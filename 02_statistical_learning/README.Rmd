---
title: "Statistical Learning"
author: "Cristian E. Nuno"
date: "`r format( x = Sys.Date(), format = '%B %d, %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 2
    dev: png
---

```{r setup, message = FALSE, warning = FALSE}
# load necessary packages -----
library(gridExtra)
library(here)
library(scales)
library(tidyverse)

# load necessary objects ----
my.theme <- 
  theme_minimal() + 
  theme(panel.grid = element_blank()
        , plot.caption = element_text(size = 5))

islr.text <-
  "Source: ISLR (http://www-bcf.usc.edu/~gareth/ISL/data.html)"

# set dpi for all chunks ----
knitr::opts_chunk$set(dpi = 300)

# load necessary data ----
df <- 
  read_csv(here("00_raw_data", "advertising.csv")) %>%
  # drop row number column
  select(-X1)

# convert from wide to long ----
df.tidy <-
  df %>%
  gather(key = "type", value = "budget", -sales)
```


## What is Statistical Learning?

Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. 

The [`Advertising`](https://github.com/cenuno/islr_notes/blob/master/00_raw_data/advertising.csv) data set consists of the `sales` of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: `TV`, `radio`, and `newspaper`.

While it isn't possible for the client to change the number of sales, they can control the advertising budget in each of the three media. Our goal is to adjust the advertising budget so that we may indirectly increase sales.

In this case, advertising budgets are the *input variables* - $X$ - while `sales` is the *output variable* - $Y$. 

$X$ goes by many names: predictors, independent variables, features, or sometimes just variables. The same for $Y$: response or dependent variable.

The relationship between sales as a measured by each media type's advertising budget is shown belown. 

```{r examine advertising}
# visualize all three advertising types and their relationship to sales ----
df.tidy %>%
  ggplot(aes(x = budget, y = sales, color = type)) +
  geom_point(show.legend = FALSE) + 
  geom_smooth(se = FALSE
              , method = "lm"
              , color = "black") +
  scale_x_continuous(name = "Budget", labels = dollar) +
  scale_y_continuous(name = "Sales"
                     , labels = dollar
                     , limits = c(0, 30)) +
  facet_wrap(facets = vars(type)) +
  labs(title = "Increasing the radio advertisement budget\nincreases sales at a higher rate than both TV or newspapers"
       , caption = islr.text) +
  my.theme
```


Another example of visualizing the relationship between $X$ and $Y$ uses `income` and `years of education` from the [`Income1`](https://github.com/cenuno/islr_notes/blob/master/00_raw_data/income1.csv) data set.

```{r income and education, message = FALSE, warning = FALSE}
income1 <- 
  read_csv(here("00_raw_data", "income1.csv")) %>%
  select(-X1)

base.plot <-
  income1 %>%
  ggplot(aes(x = Education, y = Income)) +
  geom_point(color = "orchid4") +
  scale_y_continuous(name = "Annual income"
                     , labels = function(i) paste0("$", i, "K")) +
  scale_x_continuous(name = "Years of education"
                     , breaks = pretty_breaks()) +
  my.theme +
  theme(plot.title = element_text(size = 8)
        , plot.subtitle = element_text(size = 6))

loess.plot <-
  base.plot + 
  geom_smooth(se = FALSE
              , color = "#898b47"
              , method = "loess") +
  geom_segment(aes(xend = Education
                   , yend = loess(Income ~ Education, data = income1)$fitted)
               , color = "#47898b") +
  labs(title = "The error terms are between the points and the fitted line"
       , subtitle = "Some errors are positive (above the line)\nand others are negative (below the line)"
       , caption = islr.text)

# display both plots side by side ----
grid.arrange(base.plot + 
               labs(title = "More years of education are associated\nwith higher annual incomes")
             , loess.plot
             , nrow = 1
             , ncol = 2)
```

In the formula $Y = f(x) + \epsilon$, $f$ represents the *systematic* information that $X$ provides about $Y$. $\epsilon$ is a random *error term* that is both independent from $X$ and has a mean of zero. 

**In essence, statistical learning refers to a set of approaches for estimating $f$**. In this chapter we outline some of the key theoretical concepts that arise in estimating $f$, as well as tools for evaluating the estimates obtained.

## Why estimate f?

There are two main reasons that we may wish to estimate $f$: *prediction* and *inference*.

### Prediction

In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. Since the error term averages to zero, we can predict $Y$ in this setting using the following formula:

$$\hat{Y} = \hat{f}(X)$$

where $\hat{f}$ represents our estimate for $f$, and $\hat{Y}$ represents the resulting prediction for $Y$. In this setting, $\hat{f}$ is often treated as a black box, in the sense that one is not typically concerned with the exact form of $f$, provided that it yields accurate predictions for $Y$.

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantitines: *reducible error* and *irreducible error*.

In general, $\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy $\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$.

But perfection is not possible. This is because $Y$ is also a function of $\epsilon$ - which by definition cannot be predicted using $X$. The quantity $\epsilon$ may contain both unmeasured variables and variation that are useful in predicting $Y$.

Since we don’t measure them, $f$ cannot use them for its prediction.Therefore, variability associated with $\epsilon$ also affects the accuracy of our predictions.

This is known as the irreducible error, because no matter how well we estimate $f$, we cannot reduce the error introduced by $\epsilon$.

**The focus of this book is on techniques for estimating $f$ with the aim of minimizing the reducible error.**

It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. *This bound is almost always unknown in practice.*

### Inference

We are often interested in understanding the way that $Y$ is affected as $X1$,...,$X_p$ change. In this situation we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$.

We instead want to understand the relationship between $X$ and $Y$, or more specifically, to understand how $Y$ changes as a function of $X1$,...,$X_p$. Now $\hat{f}$ cannot be treated as a black box, because we need to know its exact form.

The following questions now arise:

* Which predictors are associated with the response? 
    + Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.

* What is the relationship between the response and each predictor?

* Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? 

For instance, consider a company that is interested in conducting a direct-marketing campaign. The goal is to build a model that predicts how an individual will respond to the direct-marketing campaigin based on their demographics.

In this case, the demographic variables serve as predictors, and response to the marketing campaign (either positive or negative) serves as the outcome. The company is not interested in obtaining a deep understanding of the relationships between each individual predictor and the response; instead, the company simply wants an accurate model to predict the response using the predictors.

In contrast, consider the [`Advertising` data visualization](https://github.com/cenuno/islr_notes/blob/master/02_statistical_learning/README_files/figure-markdown_github/examine%20advertising-1.png) from earlier. One may be interested in answering questions such as:

* Which media contribute to sales?

* Which media generate the biggest boost in sales?

* How much increase in sales is associated with a given increase in TV advertising?

This situation falls into the inference paradigm. 

Another example involves modeling the brand of a product that a customer might purchase based on variables such as price, store location, discount levels, competition price, and so forth. 

In this situation one might really be most interested in **how each of the individual variables affects** the probability of purchase. For instance, what effect will changing the price of a product have on sales? This is an example of modeling for inference.

Finally, some modeling could be conducted both for prediction and inference. For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. 

In this case one might be interested in how the individual input variables affect the prices—that is, *how much extra will a house be worth if it has a view of the river*? This is an **inference** problem. 

Alternatively, one may simply be interested in predicting the value of a home given its characteristics: *is this house under- or over-valued*? This is a **prediction** problem.

Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating $f$ may be appropriate. 

For example, *linear models* allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. 

In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for $Y$, but this comes at the expense of a less interpretable model for which inference is more challenging.

## How Do We Estimate f?

Throughout this book, we explore many linear and non-linear approaches for estimating $f$. However, these methods generally share certain characteristics.

We will always assume that we have observed a set of $n$ different data points. These observations are called the *training data* because we will use these observations to train, or teach, our method how to estimate $f$.

Let $x_{ij}$ represent the value of the $j^{th}$ predictor, or input, for observation $i$, where $i$ = 1,2,...,$n$ and $j$ = 1,2,...,$p$. Correspondingly, let $y_i$ represent the response variable for the $i^{th}$ observation. Then our training data consist of

{($x_1$,$y_1$),($x_2$,$y_2$),...,($x_n$,$y_n$)} where $x_i$ ={($x_{i1}$,$x_{i2}$,...,$x_{ip}$)}$^T$.

Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function $f$. In other words, we want to find a function $f$ such that $Y$ $≈$ $f(X)$ for any observation $(X,Y)$.

Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric.

### Parametric Methods

Parametric methods involve a two-step model-based approach.

1. Make an assumption about the functional form, or shape, of $f$ (i.e. a linear model of $f(X)$ = $\beta_0$ + $\beta_1X_1$ + $\beta_2X_2$ +... + $\beta_pX_p$).

2. After a model has been selected, we need a procedure that uses the training data to *fit* or *train* the model. In the case of the linear model in step 1, we need to estimate the parameters $\beta_0$,$\beta_1$,...,$\beta_p$. We want to find the values of these parameters such that $Y$ $≈$ $\beta_0$ + $\beta_1X_1$ + $\beta_2X_2$ +... + $\beta_pX_p$. A common approach for fitting linear models is the *ordinary least squares* approach.

The model-based approach just described is referred to as *parametric*; it reduces the problem of estimating $f$ down to one of estimating a set of paramters.

Assuming a parametric form for $f$ simplifies the problem of estimating $f$ because it is generally much easier to estimate a set of parameters, such as $\beta_0$,$\beta_1$,...,$\beta_p$ in the linear model than it is to fit an entirely arbitrary function $f$.

The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$. If the chosen model is too far from the true $f$, then our estimate will be poor.

We can try to address this problem by choosing flexible models that can fit many different possible functional forms for $f$. But in general, fitting a more flexible model requires estimating a greater number of parameters.

These more complex models can lead to a phenomenon known as *overfitting* the data, which essentially means they follow the errors, or noise, too closely. Overfitting is an undesirable situation because the fit obtained will not yield accurate estimates of the response on new observations that were not part of the original training data set.

From the `Income` data set, a parametric approach to understanding `income` described by both `education` and `seniority` as a linear model would look like this:

$income$ $≈$ $\beta_0$ + $\beta_1 * education$ + $\beta_2 * seniority$

Since we have assumed a linear relationship between the response and the two predictors, the entire fitting problem reduces to estimating $\beta_0$,$\beta_1$, and $\beta_2$, which we do using least squares linear regression.

### Non-parametric methods

Non-parametric methods do not make explicit assumptions about the functional form of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.

Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$.

Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit the data well.

In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.

## The Trade-Off Between Prediction Accuracy and Model Interpretability

Of the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate $f$.

For example, linear regression is a relatively inflexible approach, because it can only generate linear functions. On the other hand, thin plate splines are considerably more flexible because they can generate a much wider range of possible shapes to estimate $f$.

```{r flexibility versus interpretability}
# store statistical learning models 
models <-
  tibble(method = c("Subset Selection"
                   , "Lasso"
                   , "Least Squares"
                   , "Generalized Additive Models"
                   , "Trees"
                   , "Bagging, Boosting"
                   , "Support Vector Machines")
         , flexibility = c(0.05
                           , 0.1
                           , 0.25
                           , 0.6
                           , 0.6
                           , 0.85
                           , 0.78)
         , interpretability = c(0.9
                                , 0.85
                                , 0.7
                                , 0.5
                                , 0.45
                                , 0.25
                                , 0.05))

# visualize each model's flexibility v. interpretability tradeoff -----
models %>%
  ggplot(aes(x = flexibility, y = interpretability, label = method)) +
  geom_text(color = "tan4") +
  scale_x_continuous(name = "Flexibility"
                     , limits = c(0, 1)
                     , breaks = c(0, 1)
                     , labels = c("Low", "High")) +
  scale_y_continuous(name = "Interpretability"
                     , limits = c(0, 1)
                     , breaks = c(0, 1)
                     , labels = c("Low", "High")) +
  labs(title = "Tradeoff between flexibility and interpretability amongst statistical learning methods"
       , subtitle = "In general, as the flexibility of a method increases, its interpretability decreases"
       , caption = "Source: ISLR, Chapter 2") +
  my.theme
```

The visual up above prokes a rational question: *why would we ever choose to use a more restrictive method instead of a very flexible approach?*

There are several reasons that we might prefer a more restrictive model. **If we are mainly interested in inference, then restrictive models are much more interpretable.**

For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between $Y$ and $X_1$,$X_2$,...,$X_p$.

In contrast, very flexible approaches can lead to such complicated estimates of $f$ that it is difficult to understand how any individual predictor is associated with the response.

* Least squares linear regression: relatively inflexible but is quite interpretable. 

* Lasso: relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients $\beta_0$, $\beta_1$, . . . , $\beta_p$. The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors (namely those with nonzero coefficient estimates).

* Generalized additive models (GAMs): extend the linear model to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve.

* Bagging, boosting, and support vector machines: fully non-linear methods with non-linear kernels are highly flexible approaches that are harder to interpret. 

We have established that when **inference** is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.

## Supervised Versus Unsupervised Learning

Most statistical learning problems fall into one of two categories: supervised or unsupervised. 

The supervised learning domain requires that for each observation of the predictor measurement(s) $x_i$, $i$ $=$ $1$,...,$n$ there is an associated response measurement $y_i$. Here, the goal is to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (**prediction**) or better understanding the relationship between the response and the predictors (**inference**). Many classical statistical learning methods such as linear regression and logistic regression, as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain.

Unsupervised learning requires that for every observation $i$ $=$ $1$,...,$n$, we observe a vector of measurements $x_i$ but no associated response $y_i$. It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis.

We can seek to understand the relationships between the variables or between the observations using unsupervised learning techniques. 

One statistical learning tool that we may use in this setting is cluster analysis, or clustering. The goal of cluster analysis
is to ascertain, on the basis of $x_1$,...,$x_n$, whether the observations fall into relatively distinct groups.

Many problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of $n$ observations. For $m$ of the observations, where $m$ $<$ $n$, we have both predictor measurements and a response measurement. For the remaining $n − m$ observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a *semi-supervised* learning problem. This topic is beyond the scope of this book.


## Session Info

```{r session info}
sessioninfo::session_info()
```
