---
title: "Linear Regression"
author: "Cristian E. Nuno"
date: "`r format( x = Sys.Date(), format = '%B %d, %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 3
    dev: png
---

```{r setup, message = FALSE, warning = FALSE}
# load necessary packages -----
library(broom)
library(gridExtra)
library(here)
library(ISLR)
library(mgcv)
library(scales)
library(splines)
library(tidyverse)

# load necessary functions ----
source(here("00_functions", "01_build_single_models.R"))

# load necessary objects ----
my.theme <- 
  theme_minimal() + 
  theme(panel.grid = element_blank()
        , plot.caption = element_text(size = 5))

islr.text <-
  "Source: ISLR (http://www-bcf.usc.edu/~gareth/ISL/data.html) | made by @cenuno_"

wage.text <- "Source: Wage and other data for a group of 3000 male workers in the Mid-Atlantic region | made by @cenuno_"

chapter.text <-
  "Source: ISLR, Chapter 3 | made by @cenuno_"

bright.blue   <- "#1e5efb"
bright.yellow <- "#edd830"

# set dpi for all chunks ----
knitr::opts_chunk$set(dpi = 300)
```

This chapter is about *linear regression*, a very simple approach for supervised learning. In particular, **linear regression is a useful tool for predicting a quantitative response**.

Linear regression has been around for a long time and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters of this book, linear regression is still a useful and widely used statistical learning method.

Moreover, it serves as a good jumping-off point for newer approaches: as we will see in later chapters, many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. *Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated*.

In this chapter, we review some of the key ideas underlying the linear regression model, as well as the least squares approach that is most commonly used to fit this model.

Recall the `Advertising` data from Chapter 2.

![](https://github.com/cenuno/islr_notes/raw/master/02_statistical_learning/README_files/figure-markdown_github/examine%20advertising-1.png)

Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? 

Here are a few important questions that we might seek to address:

1. Is there a relationship between advertising budget and sales?
    + Our first goal should be to determine whether the data provide evidence of an association between advertising expenditure and sales. *If the evidence is weak, then one might argue that no money should be spent on advertising*!
    
2. How strong is the relationship between advertising budget and sales?
    + Assuming that there is a relationship between advertising and sales, we would like to know the strength of this relationship. 
    + **In other words, given a certain advertising budget, can we predict sales with a high level of accuracy?** This would be a strong relationship. 
    + Or is a prediction of sales based on advertising expenditure only slightly better than a random guess? This would be a weak relationship.
    
3. Which media contribute to sales?
    + Do all three media—TV, radio, and newspaper—contribute to sales, or do just one or two of the media contribute? 
    + To answer this question, we must *find a way to separate out the individual effects of each medium when we have spent money on all three media*.
    
4. How accurately can we estimate the effect of each medium on sales? 
    + For every dollar spent on advertising in a particular medium, by what amount will sales increase? 
    + How accurately can we predict this amount of increase?
    
5. How accurately can we predict future sales?
    + For any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?
    
6. Is the relationship linear?
    + If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. 
    + If not, then it may still be possible to transform the predictor or the response so that linear regression can be used.
    
7. Is there synergy among the advertising media?
    + Perhaps spending $50,000 on television advertising and $50,000 on radio advertising results in more sales than allocating $100,000 to either television or radio individually. 
    + In marketing, this is known as a synergy effect, while in statistics it is called an *interaction effect*.
    
It turns out linear regression can be used to answer each of these questions. We will first discuss all of these questions in a general context, and then return to them in this specific context in Section 3.4.

## Simple Linear Regression

Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$. 

Mathematically, we can write this linear relationship as:

$Y ≈ β_0 + β_1X$,

where $≈$ is read as "approximately modeled as" and saying we are regressing $Y$ onto $X$.

For example, $X$ may represent `TV` advertising and $Y$ may represent `sales`. Then we can regress `sales` onto `TV` by fitting the model:

$sales ≈ β_0 +β_1*TV$.

$β_0$ and $β_1$ are two unknown constants that represent the *intercept* and *slope* terms in the linear model. Together, $β_0$ and $β_1$ are known as the model coefficients or parameters.

Once we have used our training data to produce estimates $\hat{β}_0$ and $\hat{β}_1$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing:

$\hat{y} = \hat{β}_0 + \hat{β}_1x$,

where $\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$. Here we use a hat symbol, $ˆ$ , to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.

### Estimating the Coefficients

In practice, $β_0$ and $β_1$ are unknown. So before we can use $Y ≈ β_0 + β_1X$ to make predictions, we must use data to estimate the coefficients. Let

$(x_1,y_1)$, $(x_2,y_2)$,..., $(x_n,y_n)$

represent $n$ observation pairs, each of which consists of a measurement of $X$ and a measurement of $Y$. In the `Advertising` example, this data set consists of the `TV` advertising budget and product `sales` in $n = 200$ different markets.

Our goal is to obtain coefficient estimates $\hat{β}_0$ and $\hat{β}_1$ such that the linear model fits the available data well: that is, so that $y_i ≈ \hat{\beta}_0 + \hat{β}_1 * x_i$ for $i = 1, . . . , n$.

In other words, we want to find an intercept $\hat{β}_0$ and a slope $\hat{β}_1$ such that the resulting line is as close as possible to the $n = 200$ data points. There are a number of ways of *measuring closeness*. **However, by far the most common approach involves minimizing the least squares criterion**, and we take that approach in this chapter.

```{r first model, warning = FALSE, message = FALSE}
# load necessary data ----
df <- 
  read_csv(here("00_raw_data", "advertising.csv")) %>%
  # drop row number column
  select(-X1)

# regress TV onto sales ----
mod <- 
  lm(sales ~ TV, data = df)

# transform lm object into tibble ----
mod.tidy <- augment(mod)

# visualize -----
df %>%
  ggplot(aes(x = TV, y = sales)) +
  geom_point(color = bright.blue) +
  geom_line(data = mod.tidy
            , aes(x = TV, y = .fitted)
            , color = bright.yellow
            , size = 1.25) +
  geom_segment(data = mod.tidy
               , aes(xend = TV, yend = .fitted)
               , color = "gray65") +
  scale_x_continuous(name = "TV budget (in thousands of dollars)", labels = dollar) +
  scale_y_continuous(name = "Sales (in thousands of units)"
                     , limits = c(0, 30)) +
  labs(title = "Linear fit captures the essence of the relationship between sales and TV budget\nalthough it is somewhat deficient on the left-hand side of the plot"
       , subtitle = "Each blue point represents the data that exist between TV and sales;\nThe yellow line is the slope that minimizes the sum of the squared errors;\nEach gray line represents an error from the observed sales value and the predicted value"
       , caption = islr.text) +
  my.theme
```

Let $\hat{y}_i$ = $\hat{β}_0$ + $\hat{β}_1x_i$ be the prediction for $Y$ based on the $i_{th}$ value of $X$. Then $e_i$ = $y_i$ − $\hat{y}_i$ represents the $i_{th}$ residual—this is the difference between the $i_{th}$ observed response value and the $i_{th}$ response value that is predicted by our linear model.

We define the **residual sum of squares (RSS)** as:

$RSS = e_1^2 + e_2^2 + · · · + e_n^2$,

or equivalently as:

$RSS = (y_1 − \hat{β}_0 − \hat{β}_1x_1)^2 + (y_2 − \hat{β}_0 − \hat{β}_1x_2)^2 + ... + (y_n − \hat{β}_0 − \hat{β}_1x_n)^2$


The least squares approach chooses \hat{β}_0 and \hat{β}_1 to minimize the RSS. Using some calculus, one can show that the minimizers are:

$$\hat{\beta}_1 = \frac{\sum^n_{i = 1}(x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i = 1}(x_i - \bar{x})^2}$$

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$$,

where $$\bar{y} \equiv \frac{1}{n}\sum^n_{i = 1}y_i$$ and $$\bar{x} \equiv \frac{1}{n}\sum^n_{i = 1}x_i$$ are the sample means. In other words, the equations for $\hat{\beta}_0$ and $\hat{\beta}_1$ define the least squares coefficient estimates for simple linear regression.

The linear model up above displays the simple linear regression fit to the `Advertising` data, where \hat{β}_0 = `r round(mod$coefficients[["(Intercept)"]], digits = 2)` and \hat{β}_1 = `r format(mod$coefficients[["TV"]], digits = 4)`. In other words, according to this approximation, an additional \$1,000 spent on TV advertising is associated with with selling an approximately `r round(1000 * mod$coefficients[["TV"]], digits = 1)` additional units of the product.

### Assessing the Accuracy of the Coefficient Estimates

Recall from Chapter 2 that we assume that the true relationship between $X$ and $Y$ takes the form $Y$ = $f(X)$ + $\epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as:

$$Y = \beta_0 + \beta_1X + \epsilon$$.

Here $β_0$ is the intercept term—that is, the expected value of $Y$ when $X = 0$, and $β_1$ is the slope—the average increase in $Y$ associated with a one-unit increase in $X$.

The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error. We typically assume that the error term is independent of $X$.

$$Y = \beta_0 + \beta_1X + \epsilon$$ defines the population regression line, which is the best linear approximation to the true relationship between $X$ and $Y$. The assumption of linearity is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true relationship is linear.

The least squares regression coefficient estimates for \hat{\beta}_0 and \hat{\beta}_1 characterize the least squares line.

At first glance, the difference between the population regression line and the least squares line may seem subtle and confusing. We only have one data set, and so what does it mean that two different lines describe the relationship between the predictor and the response?

Fundamentally, the **concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population**. For example, suppose that we are interested in knowing the population mean $\mu$ of some random variable $Y$.

Unfortunately, $\mu$ is unknown, but we do have access to $n$ observations from $Y$, which we can write as $y_1,...,y_n$, and which we can use to estimate $\mu$. A reasonable
estimate is $\hat{\mu}$ = $\bar{y}$, where $\bar{y}$ = $\frac{1}{n}\sum^n_{i = 1}y_i$ is the sample mean. 

**The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean**.

In the same way, the unknown coefficients $β_0$ and $β_1$ in linear regression define the population regression line. We seek to estimate these unknown coefficients using $\hat{β}_0$ and $\hat{β}_1$. These coefficient estimates define the least squares line.

The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of *bias*. If we use the sample mean $\hat{\mu|$ to estimate $\mu$, this estimate is unbiased, in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$. 

What exactly does this mean?

It means that on the basis of one particular set of observations $y_1$, . . . , $y_n$, $\hat{\mu}$ might overestimate $\mu$, and on the basis of another set of observations, $\hat{\mu}$ might underestimate $\mu$. But if we could average a huge number of estimates of $\mu$ obtained from a huge number of sets of observations, then this average would *exactly* equal $\mu$.

Hence, an unbiased estimator does not *systematically* over or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $β_0$ and $β_1$ on the basis of a particular data set, then our estimates won’t be exactly equal to $β_0$ and $β_1$. But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on!

We continue the analogy with the estimation of the population mean $\mu$ of a random variable $Y$. A natural question is as follows: how accurate is the sample mean $\hat{\mu}$ as an estimate of $\mu$?

We have established that the average of $\hat{\mu}$’s over many data sets will be very close to $\mu$, but that a single estimate $\hat{\mu}$ may be a substantial underestimate or overestimate of $\mu$. How far off will that single estimate of $\hat{\mu}$ be?

In general, we answer this question by computing the *standard error* of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula:

$$Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma^2}{n}$$,

where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$ (this formula holds provided that the $n$ observations are uncorrelated). Roughly speaking, the standard error tells us the average amount that this estimate $\hat{\mu}$ differs from the actual value of $\mu$.

The equation for $SE$ also tells us how this deviation shrinks with $n$—**the more observations we have, the smaller the standard error of $\hat{\mu}$**. In a similar vein, we can wonder how close $\hat{β}_0$ and $\hat{β}_1$ are to the true values $β_0$ and $β_1$.

To compute the standard errors associated with $\hat{β}_0$ and $\hat{β}_1$, we use the following formulas:

$$SE(\hat{\beta_0})^2 = \sigma^2\bigg[\frac{1}{n} + \frac{\bar{x}^2}{\sum^n_{i = 1}(x_i - \bar{x})^2}\bigg]$$, and

$$SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum^n_{i = 1}(x_i - \bar{x})^2}$$, 

where $\sigma^2$ = $Var(\epsilon)$. For these formulas to be strictly valid, we need to assume that the errors $\epsilon_i$ for each observation are uncorrelated with common variance $\sigma^2$. This is clearly not true when regressing `TV` onto `sales`, but the formula still turns out to be a good approximation.

Notice in the formula that $SE(\hat{β}_1)$ is smaller when the $x_i$ are more spread out; intuitively we have more *leverage* to estimate a slope when this is the case. We also see that $SE(\hat{\beta}_0)$ would be the same as $SE(\hat{\mu})$ if $\bar{x}$ were zero (in which case $\hat{β}_0$ would be equal to $\bar{y}$).

In general, $\sigma^2$ is not known, but can be estimated from the data. The estimate of $\sigma$ is known as the *residual standard error*, and is given by the formula:

$$RSE = \sqrt{\frac{RSS}{n - 2}}$$.

Strictly speaking, when $\sigma$ is estimated from the data we should write $\hat{SE}(\hat{β}_1)$ to indicate that an estimate has been made, but for simplicity of notation we will drop this extra "hat".

Standard errors can be used to compute *confidence intervals*. **A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter**.

The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $β_1$ approximately takes the form:

$$\beta_1 \pm 2 * SE(\hat{\beta}_1)$$.

That is, there is approximately a 95% chance that the interval

$$\bigg[\beta_1 - 2 * SE(\hat{\beta}_1), \beta_1 + 2 * SE(\hat{\beta}_1)\bigg]$$

will contain the true value of $β_1$ (relies on the assumption that the errors are Gaussian and 2 should be replaced with the 97.5% quantile of a t-distribution with $n − 2$ degrees of freedom).

Similarly, a confidence interval for β0
approximately takes the form:

$$\beta_0 \pm 2 * SE(\hat{\beta}_0)$$.

```{r calculate standard errors, warning = FALSE, message = FALSE}
# source the standard_error() function ----
source(here("00_functions", "02_standard_error.R"))

# source the confidence_interval() function ----
source(here("00_functions", "03_confidence_interval.R"))

# calculate SE for the both coeffiencents ----
se <-
  c(TRUE, FALSE) %>%
  set_names(c("intercept", "input")) %>%
  map(.f = ~ standard_error(model = mod
                            , input = "TV"
                            , intercept = .x))

# calculate lower and upper bounds of the coefficients ----
confidence.interval <-
  se %>%
  map2(.y = coefficients(mod)
       , .f = ~ confidence_interval(coeff = .y
                                    , std.error = .x
                                    , level = 0.95
                                    , df = mod$df.residual))

# store estimates of both intercept and input ----
response.unit <- 1000

intercept.estimates <-
  confidence.interval$intercept %>%
  round(digits = 2) %>%
  magrittr::multiply_by(response.unit) %>%
  prettyNum(big.mark = ",")

input.estimates <-
  confidence.interval$input %>%
  round(digits = 3) %>%
  magrittr::multiply_by(response.unit)
```

In the case of the advertising data, the 95% confidence interval for $\beta_0$ is [`r paste(round(confidence.interval$intercept, digits = 3), collapse = ", ")`] and the 95 % confidence interval for $\beta_1$ is [`r paste(round(confidence.interval$input, digits = 3), collapse = ", ")`]. The interpretation of the confidence interval requires that we understand the units of the input and response variable. 

Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between `r intercept.estimates[["lower"]]` and `r intercept.estimates[["upper"]]` units. Furthermore, for each $`r comma(response.unit)` increase in television advertising, there will be an average increase in sales of between `r input.estimates[["lower"]]` and `r input.estimates[["upper"]]` units.


## Session Info

```{r session info}
sessioninfo::session_info()
```
